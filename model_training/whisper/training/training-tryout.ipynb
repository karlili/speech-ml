{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee342d21",
   "metadata": {},
   "source": [
    "the original notebook is from hugging face colab notebook here (https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb)\n",
    "\n",
    "\n",
    "Make sure you have the following dependencies installed in your environment\n",
    "\n",
    "```\n",
    "pip install datasets transformers evaulate jiwer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1234f2",
   "metadata": {},
   "source": [
    "the common voice dataset is coming from here (https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0/viewer/zh-HK/train?p=84)\n",
    "\n",
    "It is part of mozilla foundation common voice project\n",
    "\n",
    "----\n",
    "\n",
    "We could use this format to prepare our own dataset to fine tune our version of whisper\n",
    "\n",
    "----\n",
    "\n",
    "If you want to re-use / avoid to download the voice file every time, you can un-comment the part which specify `cache_dir` and point it to the directory you want those file to be downloaded / already downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1e8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install dataset transformers evaluate librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13205eb410405fa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T20:09:39.997469Z",
     "start_time": "2023-11-21T20:02:41.345262Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for mozilla-foundation/common_voice_16_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_16_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for mozilla-foundation/common_voice_16_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_16_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 5644\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 2581\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append(datasets_dir)\n",
    "\n",
    "# before downloading any new dataset, \n",
    "# make sure to check if it needs to Check and Agrees to the terms first, otherwise the download would fail\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset_name = \"mozilla-foundation/common_voice_16_0\"\n",
    "language_to_train = 'yue'\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"train+validation\",\n",
    "  \n",
    "  )\n",
    "\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "  dataset_name, language_to_train, \n",
    "  split=\"test\",  \n",
    "  \n",
    "  )\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93cb70f8c8df1663",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !pip install \"tokenizers>=0.14,<0.15\"\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  # cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/feature\"\n",
    "  ) # start with the whisper small checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db962245-70f8-4288-9d9d-dc6144d90cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"cantonese\", \n",
    "task=\"transcribe\",\n",
    "# cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "002a1f20-0b5a-4861-92bb-ee5aceb2f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "'''\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", \n",
    "language=\"None\", \n",
    "task=\"transcribe\",\n",
    "# cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/processor\"\n",
    ")\n",
    "'''\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=None, task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d8a338c-6ce8-4b08-866f-dad714ba24eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': '8ae8769d84844c135e7b9a731c45fb69a9bee11fb651e6448723cfd7caf6207d553784a677aa9446f1f769f6b85bddf27ef9081fb22aab7ee883ba68ad5c720c', 'path': '/home/sagemaker-user/.cache/huggingface/datasets/downloads/extracted/4d40186357d4497d7a779b6630c9e370aabb350719797f607a3b11809c7624e2/yue_train_0/common_voice_yue_31492601.mp3', 'audio': {'path': '/home/sagemaker-user/.cache/huggingface/datasets/downloads/extracted/4d40186357d4497d7a779b6630c9e370aabb350719797f607a3b11809c7624e2/yue_train_0/common_voice_yue_31492601.mp3', 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 16000}, 'sentence': 'ç‚ºä½ æœªä¾†æŠ•è³‡', 'up_votes': 2, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'yue', 'segment': '', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "# Preparing Data\n",
    "\n",
    "# Whisper expecting the audio to be at sampling rate @16000 - this is just to make sure the sampling rate fits whisper's training\n",
    "# Since our input audio is sampled at 48kHz, we need to downsample it to 16kHz prior to passing it to the Whisper feature extractor, \n",
    "# 16kHz being the sampling rate expected by the Whisper model.\n",
    "from datasets import Audio\n",
    "raw_common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "print(raw_common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd3747",
   "metadata": {},
   "source": [
    "prepare the dataset\n",
    "doing the encoding -> preparing the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e69ad91a-725f-40a6-8dfd-7e3c64af477a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe79f4f4a0f4bf8aa77a819eb8a090e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5644 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-06 08:52:50.132491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-06 08:52:50.150732: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mtokenizer(batch[\u001b[39m\"\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39minput_ids\n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m finalized_common_voice \u001b[39m=\u001b[39m raw_common_voice\u001b[39m.\u001b[39;49mmap(prepare_dataset, \n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m   remove_columns\u001b[39m=\u001b[39;49mraw_common_voice\u001b[39m.\u001b[39;49mcolumn_names[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m], \n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m   num_proc\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(finalized_common_voice)\n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Initialize the accelerator\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# accelerator = Accelerator()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Move the model and dataset to the device\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://akdoxhkhb485q4b.studio.us-east-1.sagemaker.aws/home/sagemaker-user/langauge_x_change/whisper/training/training-tryout.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# model, common_voice = accelerator.prepare(model, finalized_common_voice)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:868\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 868\u001b[0m     {\n\u001b[1;32m    869\u001b[0m         k: dataset\u001b[39m.\u001b[39mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[39m=\u001b[39mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[39m=\u001b[39mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[39m=\u001b[39mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[39m=\u001b[39minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[39m=\u001b[39mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[39m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[39m=\u001b[39mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[39m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[39m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[39m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[39m=\u001b[39mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[39m=\u001b[39mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[39m=\u001b[39mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[39m=\u001b[39mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[39m=\u001b[39mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:869\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[39mif\u001b[39;00m cache_file_names \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     cache_file_names \u001b[39m=\u001b[39m {k: \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m}\n\u001b[1;32m    867\u001b[0m \u001b[39mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    868\u001b[0m     {\n\u001b[0;32m--> 869\u001b[0m         k: dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m    870\u001b[0m             function\u001b[39m=\u001b[39;49mfunction,\n\u001b[1;32m    871\u001b[0m             with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[1;32m    872\u001b[0m             with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[1;32m    873\u001b[0m             input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[1;32m    874\u001b[0m             batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[1;32m    875\u001b[0m             batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    876\u001b[0m             drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[1;32m    877\u001b[0m             remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[1;32m    878\u001b[0m             keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[1;32m    879\u001b[0m             load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[1;32m    880\u001b[0m             cache_file_name\u001b[39m=\u001b[39;49mcache_file_names[k],\n\u001b[1;32m    881\u001b[0m             writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[1;32m    882\u001b[0m             features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m    883\u001b[0m             disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[1;32m    884\u001b[0m             fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[1;32m    885\u001b[0m             num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m    886\u001b[0m             desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[1;32m    887\u001b[0m         )\n\u001b[1;32m    888\u001b[0m         \u001b[39mfor\u001b[39;00m k, dataset \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    889\u001b[0m     }\n\u001b[1;32m    890\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3197\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3191\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpawning \u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m processes\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3192\u001b[0m \u001b[39mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3193\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m examples\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3194\u001b[0m     total\u001b[39m=\u001b[39mpbar_total,\n\u001b[1;32m   3195\u001b[0m     desc\u001b[39m=\u001b[39m(desc \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mMap\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (num_proc=\u001b[39m\u001b[39m{\u001b[39;00mnum_proc\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   3196\u001b[0m ) \u001b[39mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3197\u001b[0m     \u001b[39mfor\u001b[39;00m rank, done, content \u001b[39min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3198\u001b[0m         pool, Dataset\u001b[39m.\u001b[39m_map_single, kwargs_iterable\u001b[39m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3199\u001b[0m     ):\n\u001b[1;32m   3200\u001b[0m         \u001b[39mif\u001b[39;00m done:\n\u001b[1;32m   3201\u001b[0m             shards_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/utils/py_utils.py:656\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    654\u001b[0m             pool_changed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    655\u001b[0m             \u001b[39m# One of the subprocesses has died. We should not wait forever.\u001b[39;00m\n\u001b[0;32m--> 656\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    657\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mOne of the subprocesses has abruptly died during map operation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTo debug the error, disable multiprocessing.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m             )\n\u001b[1;32m    660\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    662\u001b[0m         \u001b[39m# we get the result in case there's an error to raise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing."
     ]
    }
   ],
   "source": [
    "'''def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    print()\n",
    "    # encode target text to label ids\n",
    "    \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "'''\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    processor.tokenizer.set_prefix_tokens(language='chinese', task=\"transcribe\") \n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "finalized_common_voice = raw_common_voice.map(prepare_dataset, \n",
    "  remove_columns=raw_common_voice.column_names[\"train\"], \n",
    "  num_proc=2)\n",
    "print(finalized_common_voice)\n",
    "\n",
    "# Initialize the accelerator\n",
    "# accelerator = Accelerator()\n",
    "\n",
    "# Move the model and dataset to the device\n",
    "# model, common_voice = accelerator.prepare(model, finalized_common_voice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59378d24",
   "metadata": {},
   "source": [
    "the following is the actual training and evaluation of the model\n",
    "\n",
    "using the trainer provided by huggingface\n",
    "\n",
    "Evaluation metrics: during evaluation, we want to evaluate the model using the word error rate (WER) metric. We need to define a compute_metrics function that handles this computation.\n",
    "\n",
    "Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
    "\n",
    "Define the training configuration: this will be used by the ðŸ¤— Trainer to define the training schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2be9c5d3-35df-4995-aa04-eea6fa1b1dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87322a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acfbcd",
   "metadata": {},
   "source": [
    "Evaluation using hugging face metric - WER (Word error rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3bcbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da949c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "  \"openai/whisper-small\", \n",
    "  # cache_dir=\"/Volumes/BACKUP/Coding/HUGGING_FACE/models\"\n",
    "  )\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c08eb7b",
   "metadata": {},
   "source": [
    "What should be the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a nice youtube video guide / introduction for how to use tensorboard (https://www.youtube.com/watch?v=VJW9wU-1n18&t=4s)\n",
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02e8fe62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpoppysmic\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kenny/Coding/langauge_x_change/whisper/training/wandb/run-20231223_215706-7wxt3qhq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">sparkling-tree-2</a></strong> to <a href='https://wandb.ai/poppysmic/language-x-change' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/poppysmic/language-x-change' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x4e4671ee0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"language-x-change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d97c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import datetime\n",
    "# from accelerate import Accelerator\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H%M\")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"model/whisper-small-cantonese_\"+now,  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=500,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=False,  # if we are not using CUDA or non graphics card, use fp16=false\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\",\"wandb\"], #this would requires the tensorboardx to be installed\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=finalized_common_voice[\"train\"],\n",
    "    eval_dataset=finalized_common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    # checkpoint_activations=True\n",
    ")\n",
    "\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e159c",
   "metadata": {},
   "source": [
    "The actual Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc840b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "  5%|â–Œ         | 25/500 [02:42<49:22,  6.24s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0521, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 50/500 [05:18<46:54,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6707, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 75/500 [07:55<44:45,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.995, 'learning_rate': 1.5e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 100/500 [10:32<41:39,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3957, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 20%|â–ˆâ–ˆ        | 100/500 [31:47<41:39,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25670018792152405, 'eval_wer': 356.92307692307696, 'eval_runtime': 1274.5735, 'eval_samples_per_second': 2.012, 'eval_steps_per_second': 0.252, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 25%|â–ˆâ–ˆâ–Œ       | 125/500 [34:29<38:57,  6.23s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.193, 'learning_rate': 2.5e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 150/500 [37:03<35:56,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.157, 'learning_rate': 3e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 175/500 [39:40<35:42,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1427, 'learning_rate': 3.5e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [42:13<30:51,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1412, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [1:01:06<30:51,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17312493920326233, 'eval_wer': 301.8076923076923, 'eval_runtime': 1132.7636, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 0.283, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 225/500 [1:03:54<28:40,  6.26s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1466, 'learning_rate': 4.5e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 250/500 [1:18:44<2:16:47, 32.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1434, 'learning_rate': 5e-06, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 275/500 [1:36:23<23:24,  6.24s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1218, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [1:39:22<41:47, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1358, 'learning_rate': 6e-06, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [3:01:50<41:47, 12.54s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1573217809200287, 'eval_wer': 129.26923076923075, 'eval_runtime': 4947.7065, 'eval_samples_per_second': 0.518, 'eval_steps_per_second': 0.065, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 325/500 [3:04:38<18:58,  6.50s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1332, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 350/500 [3:22:50<17:35,  7.03s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.129, 'learning_rate': 7e-06, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 375/500 [3:41:53<1:55:47, 55.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0777, 'learning_rate': 7.500000000000001e-06, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 400/500 [3:44:29<10:12,  6.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0701, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 400/500 [5:22:26<10:12,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1577225923538208, 'eval_wer': 83.96153846153847, 'eval_runtime': 5877.0947, 'eval_samples_per_second': 0.436, 'eval_steps_per_second': 0.055, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/BACKUP/Python_VENV/langauge_x_change/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 425/500 [6:03:25<2:48:41, 134.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0823, 'learning_rate': 8.5e-06, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 450/500 [6:05:58<05:06,  6.13s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0756, 'learning_rate': 9e-06, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 475/500 [6:08:36<02:41,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0904, 'learning_rate': 9.5e-06, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [6:42:36<00:00, 288.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0822, 'learning_rate': 0.0, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [8:01:09<00:00, 288.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15555481612682343, 'eval_wer': 82.11538461538461, 'eval_runtime': 4711.569, 'eval_samples_per_second': 0.544, 'eval_steps_per_second': 0.068, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [8:01:16<00:00, 57.75s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 28876.5739, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.017, 'train_loss': 0.35178972697257993, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.35178972697257993, metrics={'train_runtime': 28876.5739, 'train_samples_per_second': 0.277, 'train_steps_per_second': 0.017, 'train_loss': 0.35178972697257993, 'epoch': 1.42})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d8672e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–‚â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–â–â–‡â–ˆâ–†</td></tr><tr><td>eval/samples_per_second</td><td>â–‡â–ˆâ–â–â–</td></tr><tr><td>eval/steps_per_second</td><td>â–‡â–ˆâ–â–â–</td></tr><tr><td>eval/wer</td><td>â–ˆâ–‡â–‚â–â–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/learning_rate</td><td>â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–„â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/total_flos</td><td>â–</td></tr><tr><td>train/train_loss</td><td>â–</td></tr><tr><td>train/train_runtime</td><td>â–</td></tr><tr><td>train/train_samples_per_second</td><td>â–</td></tr><tr><td>train/train_steps_per_second</td><td>â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.15555</td></tr><tr><td>eval/runtime</td><td>4711.569</td></tr><tr><td>eval/samples_per_second</td><td>0.544</td></tr><tr><td>eval/steps_per_second</td><td>0.068</td></tr><tr><td>eval/wer</td><td>82.11538</td></tr><tr><td>train/epoch</td><td>1.42</td></tr><tr><td>train/global_step</td><td>500</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0822</td></tr><tr><td>train/total_flos</td><td>2.30522017775616e+18</td></tr><tr><td>train/train_loss</td><td>0.35179</td></tr><tr><td>train/train_runtime</td><td>28876.5739</td></tr><tr><td>train/train_samples_per_second</td><td>0.277</td></tr><tr><td>train/train_steps_per_second</td><td>0.017</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-tree-2</strong> at: <a href='https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq' target=\"_blank\">https://wandb.ai/poppysmic/language-x-change/runs/7wxt3qhq</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231223_215706-7wxt3qhq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef21b87e",
   "metadata": {},
   "source": [
    "if you need to push the model to hugging face hub, run the following block\n",
    "\n",
    "```\n",
    "pip install --upgrade huggingface_hub\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is optional. but it would allow you to upload the model to hugging face space later on\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a8fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to push\n",
    "\n",
    "#the following arguments are needed only when we are pushing the model to hugging face hub\n",
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: hi, split: test\",\n",
    "    \"language\": \"Cantonese\",\n",
    "    \"model_name\": \"[language-x-change] Custom Whisper for Cantanese\",  # a 'pretty' name for our model\n",
    "    \"finetuned_from\": \"openai/whisper-small\",\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "    \"tags\": \"hf-asr-leaderboard\",\n",
    "}\n",
    "\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895521a",
   "metadata": {},
   "source": [
    "The following is only needed when we want to deploy a runnable version with our uploaded model on hugging face spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a427339",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2582a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "pipe = pipeline(model=\"your-own-model\")  # change to \"your-username/the-name-you-picked\"\n",
    "\n",
    "def transcribe(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return text\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Whisper Small Hindi\",\n",
    "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531bd73",
   "metadata": {},
   "source": [
    "to use the model we just compiled (https://huggingface.co/docs/transformers/tasks/asr#inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a6db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] - {'text': ' ä¸­ä¸Šç’°å¢ƒåŠç”Ÿä¸€å¸¶é™¤å’—ç•€äººå€‹æ„Ÿè¦ºéˆæ€§ä¹‹å¤–å‘¢åº¦å˜…å»ºç¯‰ç‰©äº¦éƒ½ä¿‚æ–°èˆŠäº¤ç”¨ã—Žå””è¬›ä½ å””çŸ¥å‘¢åº¦æ›¾ç¶“å‡ºç¾éŽä¸€å€‹å«åšä¸‰åé–“å˜…ç¤¾å€å•²'}\n",
      "[1] - {'text': 'è¢«ç¨±ç‚ºä¸‰åé–“å˜…ç¤¾å€ï¼Œç›®å‰ä¸¦å†‡å®Œæ•´å˜…æ–‡ç»è¨˜éŒ„è³‡æ–™ï¼Œä¸Šå‚³å–ºåä¹ä¸–ç´€ï¼Œå¸‚å–®é “è¡—åŒåŸ‹å¿…åˆ—è€…äº‹è¡—é™„è¿‘ä¸€å¸¶å˜…è¯äººèšå±…åœ°ï¼Œæ‰€å»ºé€ å˜…ä¸‰åé–“çŸ³å±‹ç§»å¾—å'}\n",
      "[2] - {'text': 'éš¨è‘—ç¤¾æœƒè®Šé· çŸ³å±‹å·²ç¶“ä¸å¾©å­˜åœ¨ç¾æ™‚ ç•ªéŸ‹å…§ä»ç„¶æœ‰æ•¸åº§å¤§ç´„åœ¨1950å¹´ä»£å»ºæˆçš„å”æµå»ºç¯‰åˆ†åˆ¥æ˜¯åœ¨2019å¹´ç¢ºå®šæˆç‚ºäºŒç´šæ­·å²å»ºç¯‰çš„å²ä¸¹é “è¡—88åŠ90è™ŸåŒåŸ‹å¹³ç´šæœ‰å¾…è©•ä¼°çš„è¯é¡æ–¹è¥¿å”æµå»ºç¯‰ç¾¤'}\n",
      "[3] - {'text': ' Kara,å…¶å¯¦ç•¶åˆæ²™æ‹‰é–“å€‹èµ·æºä¿‚'}\n",
      "[4] - {'text': 'å…¶å¯¦å¦‚æžœæˆ‘å€‘æ‰¾å›žè³‡æ–™çš„è©±æœ€æ—©å…¶å¯¦æˆ‘å€‘æ˜¯åœ¨ä¸€ç™¾å…«é›¶å¹´çš„æ”¿åºœæ†²å ±è¦‹åˆ°ä¸‰åé–“é€™å€‹åå­—å› ç‚ºæˆ‘å€‘ç¾åœ¨åœ¨é¦™æ¸¯çš„åœ°åœ–ä¸Šå…¶å¯¦æˆ‘å€‘éƒ½å¾ˆé›£å¯ä»¥è¦‹åˆ°ä¸‰åé–“é€™å€‹åå­—'}\n",
      "[5] - {'text': 'çœŸä¿‚çŸ¥é“å‘¢å€‹åå˜…äººå‘¢å¤§æ¦‚éƒ½å·²ç¶“åŽ»åˆ°å…­åæ­²æˆ–è€…ä»¥ä¸Šå˜…äººå…ˆè‡³æœƒè­˜å¾—ç”¨å‘¢å€‹å'}\n",
      "[6] - {'text': 'ç•¶æ™‚å…¶å¯¦ä¿‚å‘¢å€‹ä½ç½®å‘¢æ‡‰è©²å°±ä¿‚èµ·å’—å¤§ç´„ä¸‰åé–“'}\n",
      "[7] - {'text': 'å±‹å»ºç¯‰ç¾¤å¦‚æžœè‚‰çœ¼è¦‹åˆ°çš„ç—•è·¡å…¶å¯¦å¯èƒ½åªæœ‰è¿”'}\n",
      "[8] - {'text': 'å³ä¿‚å‘¢åº¦ç”Ÿä¸‹é–“è¡—åŠé­šè˜­æœƒå‘¢å€‹æ‹›ç‰Œ'}\n",
      "[9] - {'text': ' We can reflect on the past here, the Saradan area. There used to be a lot of Chinese people coming here.'}\n",
      "[10] - {'text': 'ä½™è˜­æœƒå°±ä¿‚æ¯å¹´ä¸ƒæœˆå˜…æ™‚å€™æœƒèˆ‰è¾¦ä½™è˜­æ€§æœƒå˜…çµ„ç¹”å•¦ä½¢å“‹å…¶å¯¦ä¿‚ä¸€ç­å±…æ°‘'}\n",
      "[11] - {'text': 'çµ„ç¹”å‡ºä¾†å˜…ä¸€å€‹åœ°æ–¹'}\n",
      "[12] - {'text': 'æ–¼å—æ€§æœƒ å°æ–¼ä¸€å€‹è¯äººç¤¾æœƒä¾†èªªéžå¸¸é‡è¦è¶…åˆ°ä¸€äº›å­¤é­‚é¬¼ä»¤åˆ°é€™è£¡å¯èƒ½å‚·ç ´ è¡—åŠå¯ä»¥å®‰å¿ƒä¸€é»ž é€™æ¨£çš„ä¸€å€‹å‚³çµ±ç¿’ä¿—'}\n",
      "[13] - {'text': 'å–ºè¡—é“ä½ˆå±€ä¸Šé¢ä¸‰åé–“ç¤¾å€å˜…ç‰¹è‰²ä¿‚é»žå˜…'}\n",
      "[14] - {'text': 'å¦‚æžœæƒ³ç†è§£ä¸‰åé–“ç¯„åœå…¶å¯¦æˆ‘å€‘æ‡‰è©²ç”±ä¸‹é¢çš„å£«ä¸¹é “è¡—é–‹å§‹è¨ˆç®—é‚£å…¶å¯¦æ˜¯ä¸€å€‹æ—å¿ƒ'}\n",
      "[15] - {'text': 'è·Ÿä½å°±ä¸€è·¯æ‰“ä¸ŠåŽ»åº¦ä¸Šé¢åŠå±±å˜…å …å µç¯„åœ'}\n",
      "[16] - {'text': 'ä¸­é–“è¨˜æ†¶ä¸€å€‹ç¯„åœå…¶å¯¦æˆ‘å€‘éƒ½å¯ä»¥ç†è§£ç‚ºä¸‰åé–“'}\n",
      "[17] - {'text': ' In thisç•¶ä¸­è£¡é¢å‘¢å…¶å¯¦éƒ½æœ‰å””å°‘å˜…åœ°æ–¹å…¨éƒ¨éƒ½ä¿‚ä¸€å•²æ·¨ä¿‚äººè¡Œè»Šå””å…¥å¾—å˜…åœ°æ–¹å•¦'}\n",
      "[18] - {'text': 'åŒ…æ‹¬ä¿‚ä¸€å•²æ–¹åŒåŸ‹ä½ å•¦ï¼Œä¾‹å¦‚æ‡·å¿µæ–¹è¥¿å•¦'}\n",
      "[19] - {'text': 'æˆçŽ‹è¡—äº¦éƒ½ä¿‚æ·¨ä¿‚äººè¡Œå˜…æ¨“æ¢¯åšŸå˜…æœ‰å•²äººäº¦éƒ½æœƒå°‡æ°¸ç†è¡—å‘¢'}\n",
      "[20] - {'text': ' è¨ˆè½åŽ»ä¸‰åé–“å˜…ç¯„åœ'}\n",
      "[21] - {'text': 'æ‰€ä»¥å¯ä»¥è¬›å…¶å¯¦æ²™é˜¿é–“ä¿‚ä¸€å€‹æš´è¡Œå˜…å°å€åšŸ'}\n",
      "[22] - {'text': 'å‘¢å€‹åœ°æ–¹å‘¢å…¶å¯¦å°±ä¿ç•™å’—å¥½å¤šå³ä¿‚å—°å€‹å¹´ä»£äº”åå¹´ä»£è‡³å…­åå¹´ä»£å•¦ä¸‰è‡³å››å±¤é«˜å˜…å ‚æ¨“å»ºç¯‰ç‰©'}\n",
      "[23] - {'text': 'é•·é é»žæ¨£åšå…ˆå¯ä»¥ä¿ç•™åˆ°ä¸‰åé–“ç¤¾å€æ­·å²æ•…äº‹'}\n",
      "[24] - {'text': ' The best way to keep your body warm is to use it. So the Yulunanæ€§æœƒ of the 30-storey can practice and make money.'}\n",
      "[25] - {'text': 'å€«è¿‘å¸‚å–®é “è¡—ä¸€å¸¶å˜…å ‚æ¨“ï¼ŒåŽŸæœ¬è¢«åŠƒå…¥å¸‚å€é‡å»ºå±€å–º 2003å¹´æå‡ºå˜…é‡å»ºè¨ˆåŠƒç•¶ä¸­'}\n",
      "[26] - {'text': 'å…¶å¾Œå› æ‡‰ç¤¾å€äººå£«æå‡ºä¿ç•™å»ºç¯‰ç‰©å˜…è¨´æ±‚ï¼Œä½¿å»ºå±€å–º 2020 å¹´æ”¾æ£„é‡å»ºè¨ˆåŠƒï¼Œç ”ç©¶ä¿è‚²åŒæ´»ç™¼é …ç›®å…¥é¢å˜…å»ºç¯‰ç¾¤'}\n",
      "[27] - {'text': 'é™¤å’—äº‹ä»¶å±€å‘¢å€‹æ´»åŒ–è¨ˆåŠƒï¼Œé™„è¿‘äº¦æœ‰å¤šå€‹æ´»åŒ–æ­·å²å»ºç¯‰å˜…é …ç›®ï¼ŒåŒ…æ‹¬éŒ¢ç”Ÿç‚ºè·é‡Œæ´»åº¦å‰å·²åˆ†è­¦å¯Ÿå®¿èˆå˜…åŽŸå€‰æ–¹'}\n",
      "[28] - {'text': 'åŠéŒ¢èº«ç‚ºå¿…çƒˆé®å¸‚è¡—å¸‚å ´å˜…é¦™æ¸¯æ–°èžåšè¦½é¤¨'}\n",
      "[29] - {'text': ' In the 30-storey area or outside, there are many historical buildings that have been destroyed'}\n",
      "[30] - {'text': 'ä¾‹å¦‚åŒ…æ‹¬æœ‰æ´»åŒ–æˆç‚ºåšè¦½é¤¨åˆæˆ–è€…å¯èƒ½ä¿‚åšå’—ä¸€å•²æ–‡åŒ–æ–‡å‰µå˜…åœ°æ–¹ä½†ä¿‚æˆ‘è‡ªå·±è¦ºå¾—å…¶å¯¦å†‡ä¸€å€‹æ–¹æ³•ä¸€å®šä¿‚ä¸–ç•Œé€šè¡Œæˆ–è€…å°±ä¿‚å«åšä¸€å®šä¿‚æœ€å¥½'}\n",
      "[31] - {'text': 'æœ‰é™£æ™‚å…¶å¯¦æœ‰å°‘å°‘å•†æ¥­å…ƒç´ å–ºè£é¢ï¼Œæœ‰é™£æ™‚éƒ½æœªå¿…ä¿‚ä¸€ä»¶å£žäº‹åšŸ'}\n",
      "[32] - {'text': ' Of course, there must be some legal instruments to make sure that the building can be changed as much as possible. Or we should have some better policies to help these buildings sustain.'}\n",
      "[33] - {'text': ' In the cityç™¼å±•å’Œä¿ç•™èˆŠç¤¾å€åŠèˆŠå»ºç¯‰ç‰©æ­·å²ä¹‹é–“å¯ä»¥é»žæ¨£æ‹Žåˆ°å€‹å¹³è¡¡å‘¢?å³æ˜¯åŸŽå¸‚ç™¼å±•å’Œä¿è‚²å¾žä¾†éƒ½å””æ‡‰è©²ä¿‚å°ç«‹'}\n",
      "[34] - {'text': ' ä¸€å€‹èˆŠå»ºç¯‰ç‰©å…¶å¯¦å°æ–¼ä¸€å€‹ç¤¾å€å…¶å¯¦éƒ½æœ‰ä½¢åƒ¹å€¼å•¦å¯èƒ½æœƒä¿‚'}\n",
      "[35] - {'text': 'åŒèº«ä»½åž‹åœ–æœ‰é—œä¿‚åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¸¶å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…éŠ'}\n",
      "[36] - {'text': ' ä½¢æ¾è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸ'}\n",
      "[37] - {'text': ' you'}\n",
      "[38] - {'text': ' The list of the most common sugar-coated hot-blooded public relations has been completed and the city council also said that it will also enter the shared-court-led public housing system. Hopefully, on the other hand, this place can become a smart, unique and lively community.'}\n",
      "[39] - {'text': ' you'}\n",
      "[40] - {'text': ' you'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "def write_contents_to_file(content): \n",
    "    now = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    json_object = json.dumps(result, indent=4)\n",
    "    with open('output/'+now+\".json\", \"w\") as f:\n",
    "        f.write(json_object)\n",
    "\n",
    "path = \"model/whisper-small-cantonese_23-12-2023-2157/checkpoint-400\"\n",
    "processor_path = \"model/whisper-small-cantonese_23-12-2023-2157\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "   path, \n",
    "   local_files_only=True,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(processor_path)\n",
    "\n",
    "transcriber = pipeline(\"automatic-speech-recognition\", \n",
    "    model=model,  \n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    # chunk_length_s=5,\n",
    "    max_new_tokens=500,\n",
    "   #  batch_size=16,\n",
    "    # return_timestamps=True\n",
    "   )\n",
    "transcriber.tokenizer.get_decoder_prompt_ids(language='cantonese', task=\"transcribe\")\n",
    "\n",
    "# file_list = [\"Audio1_2.mp3\",\"Audio1_4.mp3\",\"Audio1_5.mp3\",\"Audio1_9.mp3\",\"Audio1_10.mp3\",\"Audio1_11.mp3\"]\n",
    "# for index, file in enumerate(file_list):\n",
    "#     result = transcriber(\"source/\"+file)\n",
    "#     write_contents_to_file(result)\n",
    "#     # also it will print out the result in the following output block\n",
    "#     print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "num_of_chunks = 41\n",
    "file_prefix = \"chunk\";\n",
    "file_suffix = \".mp3\"\n",
    "\n",
    "for index in range(0, num_of_chunks):\n",
    "    result = transcriber(\"source/rthk/\"+file_prefix+str(index)+file_suffix)\n",
    "    # write_contents_to_file(result)\n",
    "    print(f'[{index}] - {result}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb5b8e",
   "metadata": {},
   "source": [
    "install the following dependencies for plotting and tabulation\n",
    "\n",
    "```\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b202c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0, 1.4)</td>\n",
       "      <td>å…¶å¯¦éƒ½æœ‰ä½¢å˜…åƒ¹å€¼å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(1.4, 4.68)</td>\n",
       "      <td>å¯èƒ½æœƒä¿‚åŒèº«ç²‰èªåŒæœ‰é—œä¿‚å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(4.68, 8.0)</td>\n",
       "      <td>åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¤§å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…æ¸¸å•¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(8.0, 15.04)</td>\n",
       "      <td>ä½¢éš•è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸå˜…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(19.04, 22.64)</td>\n",
       "      <td>å¸‚å–®å®šåŠ ä¸€å¤§å˜…å”å·žæ´»ç™¼å·¥ç¨‹å·²ç¶“å®Œæˆå˜…å˜ž</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(22.64, 26.88)</td>\n",
       "      <td>è€Œå¸‚å»ºå…±äº¦éƒ½è©±åšŸç·Šæœƒå¼•å…¥å…±åŒç§Ÿä½å–®ä½å˜…å…±å±…æ¨¡å¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.0, 4.32)</td>\n",
       "      <td>å¸Œæœ›åº•æ™‚å‘¢åº¦å‘¢å°±å¯ä»¥è®Šæˆä¸€å€‹å……æ»¿æ–‡åŒ–ç‰¹è‰²åŒåŸ‹æ´»åŠ›å˜…ç¤¾å€</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp                             text\n",
       "0      (0.0, 1.4)                        å…¶å¯¦éƒ½æœ‰ä½¢å˜…åƒ¹å€¼å•¦\n",
       "1     (1.4, 4.68)                    å¯èƒ½æœƒä¿‚åŒèº«ç²‰èªåŒæœ‰é—œä¿‚å•¦\n",
       "2     (4.68, 8.0)             åˆæˆ–è€…å¯èƒ½ä½¢æœƒå¤§å‹•åˆ°ä¸€å€‹åœ°æ–¹å˜…æ–‡åŒ–æ—…æ¸¸å•¦\n",
       "3    (8.0, 15.04)  ä½¢éš•è—ä½åŒåŸ‹ä½¢å°æ–¼å‘¢å€‹ç¤¾æœƒå‰µé€ ç·Šå˜…åƒ¹å€¼å…¶å¯¦éƒ½ä¿‚å¥½é‡è¦å˜…å…ƒç´ åšŸå˜…\n",
       "4  (19.04, 22.64)              å¸‚å–®å®šåŠ ä¸€å¤§å˜…å”å·žæ´»ç™¼å·¥ç¨‹å·²ç¶“å®Œæˆå˜…å˜ž\n",
       "5  (22.64, 26.88)          è€Œå¸‚å»ºå…±äº¦éƒ½è©±åšŸç·Šæœƒå¼•å…¥å…±åŒç§Ÿä½å–®ä½å˜…å…±å±…æ¨¡å¼\n",
       "6     (0.0, 4.32)      å¸Œæœ›åº•æ™‚å‘¢åº¦å‘¢å°±å¯ä»¥è®Šæˆä¸€å€‹å……æ»¿æ–‡åŒ–ç‰¹è‰²åŒåŸ‹æ´»åŠ›å˜…ç¤¾å€"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "df = pd.json_normalize(result, record_path =['chunks'])\n",
    "display(df)\n",
    "\n",
    "# show df in a tablular format\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdbf9d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
